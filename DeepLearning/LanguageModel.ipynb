{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModel.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNV1rHUNhNNPXpeR+10IoTo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steimel60/ML/blob/main/DeepLearning/LanguageModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVvHVs6G-6tS",
        "outputId": "f66311b7-7848-4477-b5b4-1de03d96f9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 61 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 71 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 81 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 102 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 112 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 122 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 133 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 143 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 153 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 163 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 174 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 184 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 194 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 204 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 215 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 225 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 235 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 245 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 256 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 266 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 276 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 286 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 296 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 307 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 317 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 327 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 337 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 348 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 358 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 368 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 378 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 389 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 399 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 409 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 419 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 430 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 440 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 450 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 460 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 471 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 481 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 491 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 501 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 512 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 522 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 532 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 542 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 552 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 563 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 573 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 583 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 593 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 604 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 614 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 624 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 634 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 645 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 655 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 665 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 675 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 686 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 696 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 706 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 716 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 719 kB 8.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 346 kB 69.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 38.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 197 kB 54.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 60 kB 3.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 2.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 52.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 73.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 76.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 72.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 75.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 72.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 54.6 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()\n",
        "from fastbook import *\n",
        "from fastai.text.all import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download numbers in english\n",
        "path = untar_data(URLs.HUMAN_NUMBERS)\n",
        "path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "3IH8q-VO_C35",
        "outputId": "8c32c38d-c6c9-482e-ec1e-d7331c42d750"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='32768' class='' max='30252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      108.32% [32768/30252 00:00<00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('/root/.fastai/data/human_numbers/valid.txt'),Path('/root/.fastai/data/human_numbers/train.txt')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = L()\n",
        "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
        "with open(path/'valid.txt') as f: lines += L(*f.readlines())\n",
        "lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2LJUat7_fDU",
        "outputId": "f72a6b4c-7667-4998-b2a2-91db6d3258ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Concatenate into one stream\n",
        "text = ' . '.join([l.strip() for l in lines])\n",
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h4x9hbch_xk4",
        "outputId": "84e2869c-3b53-48fc-c1f6-6df52c9ed724"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize text data\n",
        "tokens = text.split(' ')\n",
        "tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYgg3CAtADGz",
        "outputId": "07c630ed-8fe1-46b1-c64d-f9c4a63f146a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Numericalize tokens\n",
        "vocab = L(*tokens).unique()\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d-djwzVALj8",
        "outputId": "102fb825-24bc-4938-efe9-da26daf96fd0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert tokens into nums\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "nums = L(word2idx[i] for i in tokens)\n",
        "nums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7MR0r1YAWF8",
        "outputId": "880e5eb3-65db-4bd4-d9c6-c5eda19f2101"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#targets in our example will be every sequence of 3 words\n",
        "L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YirGY3NDA3sE",
        "outputId": "256e3f4d-81e2-4edc-b3e3-5e8f3a71f633"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#But we want it in tensors\n",
        "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\n",
        "seqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY8JvR2qBP68",
        "outputId": "8961e6d3-db68-4f63-98a4-c9e77e9208a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create batches\n",
        "bs = 64\n",
        "cut = int(len(seqs) * .8)\n",
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=bs, shuffle=False)"
      ],
      "metadata": {
        "id": "ZJwL-06GBgeV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build our model\n",
        "class LMModel1(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden) #layer 1 - input to hidden\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)    #layer 2 - hidden to hidden\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)    #layer 3 - hidden to output\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = F.relu(self.h_h(self.i_h(x[:,0])))\n",
        "    h = h + self.i_h(x[:,1])\n",
        "    h = F.relu(self.h_h(h))\n",
        "    h = h + self.i_h(x[:,2])\n",
        "    h = F.relu(self.h_h(h))\n",
        "    return self.h_o(h)"
      ],
      "metadata": {
        "id": "7vOc765sCC8d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build and train our learner\n",
        "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "hBZ2h0bQEGR8",
        "outputId": "ffa37562-6e81-4755-b1a6-417b1e095fad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.824297</td>\n",
              "      <td>1.970941</td>\n",
              "      <td>0.467554</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.386973</td>\n",
              "      <td>1.823242</td>\n",
              "      <td>0.467554</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.417556</td>\n",
              "      <td>1.654498</td>\n",
              "      <td>0.494414</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.376440</td>\n",
              "      <td>1.650849</td>\n",
              "      <td>0.494414</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n,counts = 0, torch.zeros(len(vocab))\n",
        "for x,y in dls.valid:\n",
        "  n += y.shape[0]\n",
        "  for i in range_of(vocab): counts[i] += (y==i).long().sum()\n",
        "idx = torch.argmax(counts)\n",
        "idx, vocab[idx.item()], counts[idx].item()/n #most common index is at index 29, is \"thousand\", and always prediciting this would give us accuracy of 15%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlX7KU-PEagE",
        "outputId": "4428a38c-c220-4526-c899-5d9cc4b98f8f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(29), 'thousand', 0.15165200855716662)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A better model\n",
        "class LMModel2(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden) #layer 1 - input to hidden\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)    #layer 2 - hidden to hidden\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)    #layer 3 - hidden to output\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = 0\n",
        "    for i in range(3): #This loop is what makes this model an RNN - recurrent neural network\n",
        "      h = h+ self.i_h(x[:,i])\n",
        "      h = F.relu(self.h_h(h))\n",
        "    return self.h_o(h)"
      ],
      "metadata": {
        "id": "QZDGUs1iFImU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We should get the same results\n",
        "learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "0jzLbaNtFr38",
        "outputId": "b0ec6338-2fa2-4e98-a613-b3f4cb3e7d7f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.816274</td>\n",
              "      <td>1.964143</td>\n",
              "      <td>0.460185</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.423805</td>\n",
              "      <td>1.739964</td>\n",
              "      <td>0.473259</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.430327</td>\n",
              "      <td>1.685172</td>\n",
              "      <td>0.485382</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.388390</td>\n",
              "      <td>1.657033</td>\n",
              "      <td>0.470406</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Adding state to RNN"
      ],
      "metadata": {
        "id": "q9g-50WNR18Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This model is stateful, it remembers the activations between calls to forward\n",
        "class LMModel3(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h = 0\n",
        "  \n",
        "  def forward(self, x):\n",
        "    for i in range(3):\n",
        "      self.h = self.h + self.i_h(x[:,i])\n",
        "      self.h = F.relu(self.h_h(self.h))\n",
        "    out = self.h_o(self.h)\n",
        "    self.h = self.h.detach() #remove gradient history\n",
        "    return out\n",
        "\n",
        "  def reset(self): self.h = 0"
      ],
      "metadata": {
        "id": "-I1i7ahZSQml"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#divide samples into groups\n",
        "m = len(seqs)//bs\n",
        "m, bs, len(seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYkRPgasUaFv",
        "outputId": "5c9ec044-b1a4-423f-a433-d8ba3465ed5a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(328, 64, 21031)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Above but reindexes the groups\n",
        "def group_chunks(ds, bs):\n",
        "  m = len(ds)//bs\n",
        "  new_ds = L()\n",
        "  for i in range(m): new_ds = L(ds[i + m*j] for j in range(bs))\n",
        "  return new_ds"
      ],
      "metadata": {
        "id": "i-YNeM2gUjx1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop last batch that isnt size 64 and dont shuffle\n",
        "cut = int(len(seqs)*.8)\n",
        "dls = DataLoaders.from_dsets(\n",
        "    group_chunks(seqs[:cut], bs),\n",
        "    group_chunks(seqs[cut:], bs),\n",
        "    bs=bs, drop_last=True, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "7AiJuL3KVAEH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build learner\n",
        "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(10,3e-3) #Better results from a little history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "OEW7ZSryVbnU",
        "outputId": "cdc4f022-fb5b-4074-8d4e-e8b5ea6d1433"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.327572</td>\n",
              "      <td>3.323006</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.324659</td>\n",
              "      <td>3.301501</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.309127</td>\n",
              "      <td>3.256275</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.278579</td>\n",
              "      <td>3.200846</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.238185</td>\n",
              "      <td>3.142714</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.191905</td>\n",
              "      <td>3.086158</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.143015</td>\n",
              "      <td>3.038446</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.094622</td>\n",
              "      <td>3.004031</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.049450</td>\n",
              "      <td>2.985095</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.009567</td>\n",
              "      <td>2.979559</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sending more signal can help too"
      ],
      "metadata": {
        "id": "DdNxkDsiW8LY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do this by predicting the next word after every single word instead of every three words"
      ],
      "metadata": {
        "id": "bGd7ieU0W-6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sl = 16 #sequence length\n",
        "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+1+sl])) for i in range(0,len(nums)-sl-1,sl))\n",
        "cut = int(len(seqs)*.8)\n",
        "dls = DataLoaders.from_dsets(\n",
        "    group_chunks(seqs[:cut], bs),\n",
        "    group_chunks(seqs[cut:], bs),\n",
        "    bs=bs, drop_last=True, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "AIsUXJhfXGvK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Each element in seqs has 2 list with the second the same as the first but offset by 1\n",
        "[L(vocab[o] for o in s) for s in seqs[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7CipQ61YJyv",
        "outputId": "ef15677f-cc6c-466b-e3d9-73c4b9f5ed31"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n",
              " (#16) ['.','two','.','three','.','four','.','five','.','six'...]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adjust model to output prediction after every word\n",
        "class LMModel4(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h = 0\n",
        "  \n",
        "  def forward(self, x):\n",
        "    outs = []\n",
        "    for i in range(sl):\n",
        "      self.h = self.h + self.i_h(x[:,i])\n",
        "      self.h = F.relu(self.h_h(self.h))\n",
        "      outs.append(self.h_o(self.h))\n",
        "    self.h = self.h.detach() #remove gradient history\n",
        "    return torch.stack(outs, dim=1)\n",
        "\n",
        "  def reset(self): self.h = 0"
      ],
      "metadata": {
        "id": "TGkMF8hQYaRW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our targets are shape bs x sl but the model returns bs x sl x vocab_sz so we need to flatten before calling loss func"
      ],
      "metadata": {
        "id": "UT6czdL3agl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(inp, targ):\n",
        "  return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
      ],
      "metadata": {
        "id": "psPBV15easo-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15,3e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "nmY5oFmtbAnH",
        "outputId": "a4305e65-42ae-4e05-ddfc-98638161021d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.515865</td>\n",
              "      <td>3.534737</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.513115</td>\n",
              "      <td>3.512919</td>\n",
              "      <td>0.026367</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.504191</td>\n",
              "      <td>3.462760</td>\n",
              "      <td>0.028320</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.485763</td>\n",
              "      <td>3.390942</td>\n",
              "      <td>0.028320</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.458187</td>\n",
              "      <td>3.311083</td>\n",
              "      <td>0.042969</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.423964</td>\n",
              "      <td>3.230036</td>\n",
              "      <td>0.111328</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.385307</td>\n",
              "      <td>3.150996</td>\n",
              "      <td>0.192383</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.343780</td>\n",
              "      <td>3.076850</td>\n",
              "      <td>0.228516</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.300672</td>\n",
              "      <td>3.009414</td>\n",
              "      <td>0.261719</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.257162</td>\n",
              "      <td>2.951045</td>\n",
              "      <td>0.262695</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.214379</td>\n",
              "      <td>2.904054</td>\n",
              "      <td>0.266602</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.173331</td>\n",
              "      <td>2.869632</td>\n",
              "      <td>0.269531</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.134890</td>\n",
              "      <td>2.847794</td>\n",
              "      <td>0.271484</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.099728</td>\n",
              "      <td>2.837105</td>\n",
              "      <td>0.269531</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>3.068244</td>\n",
              "      <td>2.834272</td>\n",
              "      <td>0.270508</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save time with PyTorch"
      ],
      "metadata": {
        "id": "w6TWg1l9cZw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel5(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h = torch.zeros(n_layers, bs, n_hidden)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    res,h = self.rnn(self.i_h(x), self.h)\n",
        "    self.h = h.detach()\n",
        "    return self.h_o(res)\n",
        "\n",
        "  def reset(self): self.h.zero_()"
      ],
      "metadata": {
        "id": "7qVMN7mYccsT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel5(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "vQ8L37opdplm",
        "outputId": "b4482784-557a-4ded-84e6-9a2e917958de"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.477326</td>\n",
              "      <td>3.449765</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.472430</td>\n",
              "      <td>3.411843</td>\n",
              "      <td>0.018555</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.456496</td>\n",
              "      <td>3.323555</td>\n",
              "      <td>0.058594</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.423368</td>\n",
              "      <td>3.191411</td>\n",
              "      <td>0.326172</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.373218</td>\n",
              "      <td>3.037416</td>\n",
              "      <td>0.411133</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.310364</td>\n",
              "      <td>2.879385</td>\n",
              "      <td>0.443359</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.239548</td>\n",
              "      <td>2.730469</td>\n",
              "      <td>0.439453</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.164955</td>\n",
              "      <td>2.601913</td>\n",
              "      <td>0.442383</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.090265</td>\n",
              "      <td>2.499048</td>\n",
              "      <td>0.445312</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.018275</td>\n",
              "      <td>2.421227</td>\n",
              "      <td>0.444336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.950752</td>\n",
              "      <td>2.365153</td>\n",
              "      <td>0.443359</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.888645</td>\n",
              "      <td>2.327304</td>\n",
              "      <td>0.443359</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.832379</td>\n",
              "      <td>2.304526</td>\n",
              "      <td>0.444336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.782050</td>\n",
              "      <td>2.293667</td>\n",
              "      <td>0.445312</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.737502</td>\n",
              "      <td>2.290809</td>\n",
              "      <td>0.446289</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exploding or Disappearing activations (quickly approaching +- infinity) are likely whats giving us worse results"
      ],
      "metadata": {
        "id": "qqwy4Vt7LMMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#We can use LTSM (Long short-term memory) to help this"
      ],
      "metadata": {
        "id": "XDS56p5FL0Tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LTSMCell(Module):\n",
        "  def __init__(self, ni, nh):\n",
        "    self.forget_gate = nn.Linear(ni + nh, nh)\n",
        "    self.input_gate  = nn.Linear(ni + nh, nh)\n",
        "    self.cell_gate   = nn.Linear(ni + nh, nh)\n",
        "    self.output_gate = nn.Linear(ni + nh, nh)\n",
        "\n",
        "  def forward(self, input, state):\n",
        "    h,c = state\n",
        "    h = torch.stack([h, input], dim=1)\n",
        "    forget = torch.sigmoid(self.forget_gate(h))\n",
        "    c = c*forget\n",
        "    inp = torch.sigmoid(self.input_gate(h))\n",
        "    cell = torch.tanh(self.cell_gate(h))\n",
        "    c = c+inp*cell\n",
        "    out = torch.sigmoid(self.output_gate(h))\n",
        "    h = out * torch.tanh(c)\n",
        "    return h, (h,c)\n"
      ],
      "metadata": {
        "id": "WctGCNajMrH2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can optimize the class above\n",
        "class LTSMCell(Module):\n",
        "  def __init__(self, ni, nh):\n",
        "    self.ih = nn.Linear(ni,4*nh)\n",
        "    self.hh = nn.Linear(nh,4*nh)\n",
        "  \n",
        "  def forward(self, input, state):\n",
        "    h,c = state\n",
        "    gates = (self.ih(input)+self.hh(h)).chunk(4,1)\n",
        "    ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
        "    cellgate = gates[3].tanh()\n",
        "    c = (forgetgate*c) + (ingate*cellgate)\n",
        "    h = outgate*c.tanh()\n",
        "    return h, (h,c)"
      ],
      "metadata": {
        "id": "Qwv0Vk8cOTmj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update model to use lstm\n",
        "class LMModel6(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "  \n",
        "  def forward(self, x):\n",
        "    res,h = self.rnn(self.i_h(x), self.h)\n",
        "    self.h = [h_.detach() for h_ in h]\n",
        "    return self.h_o(res)\n",
        "\n",
        "  def reset(self):\n",
        "    for h in self.h: h.zero_()"
      ],
      "metadata": {
        "id": "Ruz6skNKPaRr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel6(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "0V4l1yglPzIY",
        "outputId": "47f3f393-7bee-4eaf-f59a-2290e49a67fb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.413969</td>\n",
              "      <td>3.410167</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.411525</td>\n",
              "      <td>3.392582</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.403557</td>\n",
              "      <td>3.347970</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.385589</td>\n",
              "      <td>3.242154</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.346944</td>\n",
              "      <td>3.061805</td>\n",
              "      <td>0.141602</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.278336</td>\n",
              "      <td>3.003152</td>\n",
              "      <td>0.146484</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.214273</td>\n",
              "      <td>2.956650</td>\n",
              "      <td>0.186523</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.158749</td>\n",
              "      <td>2.899602</td>\n",
              "      <td>0.253906</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.110103</td>\n",
              "      <td>2.842013</td>\n",
              "      <td>0.213867</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.066505</td>\n",
              "      <td>2.795560</td>\n",
              "      <td>0.272461</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.027027</td>\n",
              "      <td>2.758839</td>\n",
              "      <td>0.277344</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.991174</td>\n",
              "      <td>2.731488</td>\n",
              "      <td>0.278320</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.958683</td>\n",
              "      <td>2.713890</td>\n",
              "      <td>0.280273</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.929477</td>\n",
              "      <td>2.705119</td>\n",
              "      <td>0.280273</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.903510</td>\n",
              "      <td>2.702739</td>\n",
              "      <td>0.280273</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}